from pathlib import Path
import streamlit as st
from PIL import Image

# PATH SETTINGS
current_dir = Path(__file__).parent if "__file__" in locals() else Path.cwd()
css_file = current_dir / "styles" / "main.css"
resume_file = current_dir / "assets" / "resume.pdf"
profile_pic = current_dir / "assets" / "profile_pic.jpeg"


# GENERAL SETTINGS
PAGE_TITLE = "Digital Resume | Luis Ruiz-Santiago"
PAGE_ICON = ":wave:"
NAME = "Luis Ruiz-Santiago"
DESCRIPTION = """
I am data analyst with experience in digital transformation projects, leverage generative AI, and cloud platforms to drive data-driven decision-making and actionable insights.
"""

EMAIL = "ldruizsan@proton.me"
SOCIAL_MEDIA = {
    "LinkedIn": "https://www.linkedin.com/in/luisruiz1",
    "Github": "https://www.github.com/ldruizsan",
    "Tableau Public": "https://public.tableau.com/app/profile/luis.ruiz.santiago"
}

PROJECTS = {
    "Airline Sentiment Analysis - Streamlit app showing sentiment analysis from user tweets": {
        "link": "https://airlines-sentiment-001.streamlit.app/",
        "tags": ["Streamlit", "Python", "NLP", "Sentiment Analysis"]
    },
    "Maven Analytics Toy Store Analysis - Tableau visualization analyzing store performance": {
        "link": "https://public.tableau.com/app/profile/luis.ruiz.santiago/viz/MavenMexicoToyStoreChallenge/MavenToyCompanyinMexico",
        "tags": ["Tableau", "Data Visualization", "Analytics"]
    },
    "Beta Parameter and CAPM Financial Analysis - Jupyter notebook analyzing stock valuation": {
        "link": "https://github.com/ldruizsan/python-capm-finance",
        "tags": ["Python", "Finance", "Jupyter Notebook"]
    },
    "Data Analysis and Visualization - Jupyter notebook analyzing US pollution data": {
        "link": "https://gist.github.com/ldruizsan/56cbce888b288daefd14a27d73c8b479",
        "tags": ["Python", "Data Analysis", "Visualization", "Jupyter Notebook"]
    },
    "Cloud deployment of a web app - Automated deployment of a serverless e-commerce store using AWS CodeBuild": {
        "link": "https://github.com/ldruizsan/cloudmart",
        "tags": ["AWS", "Docker", "AWS EC2", "Serverless", "AWS EKS", "Terraform", "DynamoDB","AWS CodeBuild"]
    },
    "Marketing Dashboard - Excel analysis of marketing campaign performance and customer segmentation": {
        "link": "https://1drv.ms/x/c/e99d4a59bb58b556/EXq6iWgyrapBrLc_Xmh95SoBp23rcHxgvq3396qM4pZkmA?e=EG4Obu",
        "tags": ["Excel", "Data Analysis", "Data Visualization","Marketing"]
        }
}
st.set_page_config(page_title=PAGE_TITLE, page_icon=PAGE_ICON)
st.title("Welcome to my digital resume!")

# LOAD ASSETS AND CSS
with open(css_file) as f:
    st.markdown("<style>{}</style>".format(f.read()),unsafe_allow_html=True)
with open(resume_file,"rb") as pdf_file:
    pdfbyte = pdf_file.read()
profile_pic = Image.open(profile_pic)

# HERO SECTION
col1, col2 = st.columns(2, gap="small")
with col1:
    st.image(profile_pic, width=350)

with col2:
    st.title(NAME)
    st.write(DESCRIPTION)
    st.download_button(
        label="ðŸ“„ Download Resume",
        data = pdfbyte,
        file_name = resume_file.name,
        mime = "application/octet-stream"
    )
    st.write(EMAIL)

# SOCIAL MEDIA LINKS
cols = st.columns(len(SOCIAL_MEDIA))
for index, (platform, link) in enumerate(SOCIAL_MEDIA.items()):
    cols[index].write(f"[{platform}]({link})")

# EXPERIENCE AND QUALIFICATIONS
st.subheader("Experience & Qualifications")
st.write("""
         - âœ… Experience in digital transformation projects
         - âœ… Perform data mapping, data cleaning and validation, data analysis, and dashboards
         - âœ… Practical understanding of statistical principles to provide insights
         - âœ… Prioritize data protection
         - âœ… Certified in AWS, Azure, Databricks, and Lean Sig Sigma Yellow Belt
         """
         )
 # SKILLS
st.subheader("Skills")
st.write("""
         - Programming: Python (Pandas,Numpy, Scikit-learn), SQL
         - Data Visualization: Excel, Tableau, Plotly, Matplotlib, Seaborn, IBM Cognos
         - Databases: PostgreSQL, MongoDB, MySQL, DB2
         - Cloud platforms: AWS, Azure, Databricks, Snowflake, dbt, IBM
         - ITSM: Jira, ServiceNow
         - Generative AI: Prompt Engineering, ChatGPT, Claude, Gemini, DeepSeek, M365 Copilot
        """
)

# WORK HISTORY
st.write("---")
st.subheader("Work History")
st.write("**Technology Architect Analyst | Accenture**")
st.write("06/2022 - 12/2024")
st.write(
    """
    - Staffed across client engagements in digital transformation, cloud migration, and systems modernization for clients in the finance, fast food, and retails sectors
    - Spearheaded documentation and testing frameworks for a multiplatform SaaS application, including authoring dozens of user stories for QA teams, create a Product Playbook detailing feature entry points, cross-platform workflows, and ideal user paths to drive testing efficiency, and develop a Testing Matrix to standardize post-release validation, reduce duplication risk. Progress was tracked using Jira
    - Led data validation and cleanup for legacy system modernization, ensuring accuracy for cloud migration, identify data gaps by comparing transaction logs to application outputs
    - Revamped training documentation, and design a database workflow diagram to accelerate onboarding of client teams
    - Managed client-facing slides for sprint progress, risks, and blockers to improve transparency and maintain communication
    - Monitored 20+ Linux servers for SLA compliance to ensure uptime, resolve critical issues, and deploy patches as well as automate alerts via ServiceNow and reduce response time
    """
)

st.write("**Fulfillment Center Associate I | Amazon**")
st.write("09/2021 - 04/2022")
st.write("""
    - Ensure fast and accurate transfer of 2000-2500 items per day from inventory to packaging with a low DPMO count
    - Stow items to add them to inventory, perform simple quality test to ensure the number of items in a bin match what is shown on the website, and pack the items securely for shipment
    - Follow 5S guidelines to maintain a clean and safe work station
        """)

# PROJECTS
st.write("---")
st.subheader("Projects")
for project, details in PROJECTS.items():
    st.write(f"[{project}]({details['link']})")
    st.write("**Tools:** " + ", ".join(details["tags"]))
    st.divider()